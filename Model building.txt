Step1:
    Importing libraries such as numpy, pandas, matplotlib, scikit-learn modules, imblearn (for SMOTE), and pickle for model saving.

Step2:
    Loading dataset from CSV file and printing first 5 rows to get an overview.

Step3:
    Checking shape of DataFrame (rows and columns) which returns a tuple.

Step4:
    Checking data types of each column to understand data formats.

Step5:
    Printing summary information of the dataset (non-null counts and data types).

Step6:
    Printing statistical summary (mean, std, min, max, quartiles) of numeric columns.

Step7:
    Checking for missing values in each column to identify nulls.

Step8:
    Count the unique values in the 'Outcome' column to see the distribution of classes

Step9:
    Separating Features and Target:
        Features (X) = all columns except "Outcome"
        Target (y) = "Outcome" column indicating diabetes status.

    SMOTE Initialization:
        SMOTE is initialized to handle class imbalance by synthesizing minority class samples.

    Applying SMOTE:
        SMOTE applied on X and y to create balanced datasets (X_resampled, y_resampled).

    Before SMOTE - Class Distribution:
        Count of each class (0 or 1) before balancing is printed.

    After SMOTE - Balanced Class Distribution:
        Count of each class after balancing is printed.

Step10:
    Returns the distinct values present in the "Outcome" column.

Step11:
    Creates a groupby object that groups data by the "Outcome" column.

Step12:
    Group the data into two groups:
        Group 0: people without diabetes
        Group 1: people with diabetes
    Then calculates the mean for all other columns in each group.


Step13:
    Displays first 5 rows of the features dataset (X).

Step14:
    Displays first 5 values of the target variable (y).






Data Standardization:


Step15:
    StandardScaler from scikit-learn is initialized to standardize features.

Step16:
    The scaler is fit on the features to calculate mean and standard deviation per feature.

Step17:
    The scaler transforms features by subtracting mean and dividing by std deviation (z-score normalization).

Step18:
    Shows standardized data as a NumPy array with scaled feature values.

Step19:
    Now, X contains standardized feature data ready for machine learning.

Step20:
    Confirms X is a NumPy array holding standardized features.

Step21:
    Logistic Regression Model:
        Model is imported and initialized.
        Model is trained on X and y.
        Predictions made on training data.
        Accuracy is computed and printed to show model fit.

Step22:
    Splitting Dataset:
        train_test_split divides data into 80% training and 20% testing.
        stratify=y maintains class distribution.
        random_state=2 ensures reproducibility.






Training Model:


Step23:
    Support Vector Machine (SVM):
        SVM tries to find best separating hyperplane with maximum margin.

Step24:
    Model trained with training features and labels (X_train, y_train).

Step25:
    Model predicts labels on training data (X_train).

Step26:
    Predictions stored as an array/list.

Step27:
    Accuracy on training data calculated by comparing predicted vs actual labels.

Step28:
    Prints training accuracy percentage.

Step29:
    Model predicts on unseen test data (X_test) and prints results.

Step30:
    Calculates accuracy on test data by comparing predicted vs actual labels.

Step31:
    Prints test data accuracy showing generalization performance.





Predictive System:


Step32:
    Preprocess single input sample by reshaping and standardizing using fitted scaler.

Step33:
    Model predicts diabetes outcome for the preprocessed input.

Step34:
    Prints clear message if the person is diabetic or not based on prediction.






Random Forest Classifier:


Step35:
    Model Initialization:
        Random Forest classifier with 5 trees and max depth 5.

    Model Training:
        Trained on X_train and y_train.

    Predictions:
        Made on training data and test data.

    Accuracy Calculation:
        On training and test datasets.

    Performance Metrics:
        Precision, Recall, F1 Score, ROC-AUC calculated for train and test sets.

    Confusion Matrix:
        Displayed for both training and test predictions.






Logistic Regression (Alternative Model):


Step36:
    Model Initialization:
        Logistic Regression with max_iter=500.

    Model Training:
        Trained on training data.

    Predictions and Accuracy:
        On both train and test datasets.

    Performance Metrics:
        Precision, Recall, F1 Score, ROC-AUC for train and test sets.

    Confusion Matrix:
        Displayed for both train and test predictions.

Step37:
    Save trained model as a .pkl file for future use.
